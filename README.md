# Awesome-Video-Captioning
A curated list of research papers in Video Captioning(from 2015 to 2019). Link to the code and project website if available.

# Paper List
## 2015
1. **LSTM-P**: [Translating Videos to Natural Language Using Deep Recurrent Neural Networks](https://www.cs.utexas.edu/users/ml/papers/venugopalan.naacl15.pdf) <br>
*Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko* <br>
NAACL, 2015.[[caffe-code]](https://gist.github.com/vsubhashini/3761b9ad43f60db9ac3d)

2. **LRCN**: [Long-term Recurrent Convolutional Networks for Visual Recognition and Description](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf) <br>
*Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, Sergio Guadarrama, Kate Saenko, Trevor Darrell* <br>
CVPR, 2015.[[website]](http://jeffdonahue.com/lrcn/)

3. **S2VT**: [Sequence to Sequence – Video to Text](https://www.cs.utexas.edu/users/ml/papers/venugopalan.iccv15.pdf)<br>
*Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko*<br>
ICCV, 2015.[[caffe-code]](https://gist.github.com/vsubhashini/38d087e140854fee4b14)

4. **SA**: [Describing Videos by Exploiting Temporal Structure](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yao_Describing_Videos_by_ICCV_2015_paper.pdf)<br>
*Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, Aaron Courville*<br>
ICCV, 2015.[[theano-code]](https://github.com/yaoli/arctic-capgen-vid) [[tf-code]](https://github.com/tsenghungchen/SA-tensorflow)

## 2016
1. **LSTM-E**: [Jointly Modeling Embedding and Translation to Bridge Video and Language](http://openaccess.thecvf.com/content_cvpr_2016/papers/Pan_Jointly_Modeling_Embedding_CVPR_2016_paper.pdf)<br>
*Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, Yong Rui*<br>
CVPR, 2016.

2. **HRNE**: [Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning](http://zhongwen.ai/pdf/HRNE.pdf)<br>
*Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, Yueting Zhuang*<br>
CVPR, 2016.

3. **h-RNN**: [Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks](https://arxiv.org/pdf/1510.07712)<br>
*Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, Wei Xu*<br>
CVPR, 2016.

4. **MSR-VTT**: [MSR-VTT: A Large Video Description Dataset for Bridging Video and Language](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf)<br>
*Jun Xu , Tao Mei , Ting Yao and Yong Rui*<br>
CVPR, 2016.[[website]](https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/)

5. **BiLSTM**: [Video Description using Bidirectional Recurrent Neural Networks](https://arxiv.org/pdf/1604.03390)<br>
*Álvaro Peris, Marc Bolaños, Petia Radeva, Francisco Casacuberta*<br>
ICANN, 2016.

## 2017
1. **DenseVidCap**: [ Weakly Supervised Dense Video Captioning](https://arxiv.org/pdf/1704.01502)<br>
*Zhiqiang Shen, Jianguo Li, Zhou Su, Minjun Li, Yurong Chen, Yu-Gang Jiang, Xiangyang Xue*<br>
CVPR, 2017.[[tf-code]](https://github.com/SCLinDennis/Weakly-Supervised-Dense-Video-Captioning)

2. **LSTM-TSA**: [Video Captioning with Transferred Semantic Attributes](https://arxiv.org/pdf/1611.07675)<br>
*Yingwei Pan, Ting Yao, Houqiang Li, Tao Mei*<br>
CVPR, 2017.

3. **SCN**: [Semantic Compositional Networks for Visual Captioning](https://arxiv.org/pdf/1611.08002)<br>
*Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu, Kenneth Tran, Jianfeng Gao, Lawrence Carin, Li Deng*<br>
CVPR, 2017.[[theano-code]](https://github.com/zhegan27/Semantic_Compositional_Nets)

4. **StyleNet**: [StyleNet: Generating Attractive Visual Captions with Styles](http://openaccess.thecvf.com/content_cvpr_2017/papers/Gan_StyleNet_Generating_Attractive_CVPR_2017_paper.pdf)<br>
*Chuang Gan, Zhe Gan, Xiaodong He, Jianfeng Gao, Li Deng*<br>
CVPR, 2017.[[pytorch-code]](https://github.com/kacky24/stylenet)

5. **CT-SAN**: [End-to-end Concept Word Detection for Video Captioning, Retrieval, and Question Answering](https://zpascal.net/cvpr2017/Yu_End-To-End_Concept_Word_CVPR_2017_paper.pdf)<br>
*Youngjae Yu, Hyungjin Ko, Jongwook Choi, Gunhee Kim*<br>
CVPR, 2017.[[tf-code]](https://gitlab.com/fodrh1201/CT-SAN/tree/master)

6. **CGVS**: [Top-down Visual Saliency Guided by Captions](http://zpascal.net/cvpr2017/Ramanishka_Top-Down_Visual_Saliency_CVPR_2017_paper.pdf)<br>
*Vasili Ramanishka, Abir Das, Jianming Zhang, Kate Saenko*<br>
CVPR, 2017.[[tf-code]](https://github.com/VisionLearningGroup/caption-guided-saliency)

7. **BA**: [Hierarchical Boundary-Aware Neural Encoder for Video Captioning](http://openaccess.thecvf.com/content_cvpr_2017/papers/Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper.pdf)<br>
*Lorenzo Baraldi, Costantino Grana, Rita Cucchiara*<br>
CVPR, 2017.[[pytorch-code]](https://github.com/Yugnaynehc/banet)

8. **TDDF**: [Task-Driven Dynamic Fusion: Reducing Ambiguity in Video Description](https://www.zpascal.net/cvpr2017/Zhang_Task-Driven_Dynamic_Fusion_CVPR_2017_paper.pdf)<br>
*Xishan Zhang, Ke Gao, Yongdong Zhang, Dongming Zhang, Jintao Li,and Qi Tian*<br>
CVPR, 2017.

9. **GEAN**: [Supervising Neural Attention Models for Video Captioning by Human Gaze Data](http://zpascal.net/cvpr2017/Yu_Supervising_Neural_Attention_CVPR_2017_paper.pdf)<br>
*Youngjae Yu, Jongwook Choi, Yeonhwa Kim, Kyung Yoo, Sang-Hun Lee, Gunhee Kim*<br>
CVPR, 2017.[[tf-code]](https://github.com/yj-yu/Recurrent_Gaze_Prediction)

10. **MM-Att**: [Attention-Based Multimodal Fusion for Video Description](http://openaccess.thecvf.com/content_ICCV_2017/papers/Hori_Attention-Based_Multimodal_Fusion_ICCV_2017_paper.pdf)<br>
*Chiori Hori, Takaaki Hori, Teng-Yok Lee, Kazuhiro Sumi, John R. Hershey, Tim K. Marks*<br>
ICCV, 2017.

11. **Tessellation**: [Temporal Tessellation: A Unified Approach for Video Analysis](http://openaccess.thecvf.com/content_ICCV_2017/papers/Kaufman_Temporal_Tessellation_A_ICCV_2017_paper.pdf)<br>
*Dotan Kaufman, Gil Levi, Tal Hassner, Lior Wolf*<br>
ICCV, 2017.[[tf-code]](https://github.com/dot27/temporal-tessellation)

12. **MTEG**: [Multi-Task Video Captioning with Video and Entailment Generation](https://arxiv.org/pdf/1704.07489)<br>
*Ramakanth Pasunuru, Mohit Bansal*<br>
ACL, 2017.

13. **MAM-RNN**: [MAM-RNN: Multi-level Attention Model Based RNN for Video Captioning](https://www.ijcai.org/proceedings/2017/0307.pdf)<br>
*Xuelong Li, Bin Zhao, Xiaoqiang Lu*<br>
IJCAI, 2017.

14. **suo**: [Hierarchical LSTM with Adjusted Temporal Attention for Video Captioning](https://www.ijcai.org/proceedings/2017/0381.pdf)<br>
*Jingkuan Song, Lianli Gao, Zhao Guo, Wu Liu, Dongxiang Zhang, Heng Tao Shen*<br>
IJCAI, 2017.

## 2018
1. **Review**: [Study of Video Captioning Problem](https://www.cs.princeton.edu/courses/archive/spring18/cos598B/public/projects/LiteratureReview/COS598B_spr2018_VideoCaptioning.pdf)<br>
*Jiaqi Su*<br>
2018.

2. [Fine-grained Video Captioning for Sports Narrative](http://openaccess.thecvf.com/content_cvpr_2018/papers/Yu_Fine-Grained_Video_Captioning_CVPR_2018_paper.pdf)<br>
*Huanyu Yu, Shuo Cheng, Bingbing Ni, Minsi Wang, Jian Zhang, Xiaokang Yang*<br>
CVPR, 2018.


3. **TSA-ED**: [Interpretable Video Captioning via Trajectory Structured Localization](http://openaccess.thecvf.com/content_cvpr_2018/papers/Wu_Interpretable_Video_Captioning_CVPR_2018_paper.pdf)<br>
*Xian Wu, Guanbin Li Qingxing Cao, Qingge Ji, Liang Lin*<br>
CVPR, 2018.


4. **RecNet**: [Reconstruction Network for Video Captioning](https://www.zpascal.net/cvpr2018/Wang_Reconstruction_Network_for_CVPR_2018_paper.pdf)<br>
*Bairui Wang, Lin Ma, Wei Zhang, Wei Liu*<br>
CVPR, 2018.[[pytorch-code]](https://github.com/hobincar/reconstruction-network-for-video-captioning)


5. **M3**: [M3: Multimodal Memory Modelling for Video Captioning](http://openaccess.thecvf.com/content_cvpr_2018/papers/Wang_M3_Multimodal_Memory_CVPR_2018_paper.pdf)<br>
*Junbo Wang, Wei Wang, Yan Huang, Liang Wang, Tieniu Tan*<br>
CVPR, 2018.


6. **PickNet**: [Less Is More: Picking Informative Frames for Video Captioning](https://eccv2018.org/openaccess/content_ECCV_2018/papers/Yangyu_Chen_Less_is_More_ECCV_2018_paper.pdf)<br>
*Yangyu Chen, Shuhui Wang, Weigang Zhang, Qingming Huang*<br>
ECCV, 2018.


7. **ECO-SCN**: [ECO: Efficient Convolutional Network for Online Video Understanding](http://openaccess.thecvf.com/content_ECCV_2018/papers/Mohammadreza_Zolfaghari_ECO_Efficient_Convolutional_ECCV_2018_paper.pdf)<br>
*Mohammadreza Zolfaghari, Kamaljeet Singh, Thomas Brox*<br>
ECCV, 2018.[[caffe-code]](https://github.com/mzolfaghari/ECO-efficient-video-understanding) [[pytorch-code]](https://github.com/zhang-can/ECO-pytorch) 


8. **SibNet**: [SibNet: Sibling Convolutional Encoder for Video Captioning](https://cse.buffalo.edu/~jsyuan/papers/2018/SibNet__Sibling_Convolutional_Encoder_for_Video_Captioning.pdf)<br>
*Sheng liu, Zhou Ren, Junsong Yuan*<br>
ACM MM, 2018.

9. **TubeNet**: [Video Captioning with Tube Features](https://www.ijcai.org/proceedings/2018/0164.pdf)<br>
*Bin Zhao, Xuelong Li, Xiaoqiang Lu*<br>
IJCAI, 2018.


## 2019
1. **suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)

2. **suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


3. **suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


4. **suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


5. **suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


6. **suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


7. **suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)

8. **suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)

9. **suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)

10. **suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)

## Dense Captioning
1. **DenseEvents**: [Dense-Captioning Events in Videos](http://openaccess.thecvf.com/content_ICCV_2017/papers/Krishna_Dense-Captioning_Events_in_ICCV_2017_paper.pdf)<br>
*Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, Juan Carlos Niebles*<br>
ICCV, 2017.[[code]](https://github.com/ranjaykrishna/densevid_eval) [[website]](https://cs.stanford.edu/people/ranjaykrishna/densevid/)

2. **suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


3. **suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


4. **suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


5. **suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


6. **suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


7. **suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


8. **suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)

## Grounded Captioning
1. **GVD**: [Grounded Video Description](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhou_Grounded_Video_Description_CVPR_2019_paper.pdf)<br>
*Luowei Zhou, Yannis Kalantidis, Xinlei Chen, Jason J. Corso, Marcus Rohrbach*<br>
CVPR, 2019.[[pytorch-code]](https://github.com/facebookresearch/grounded-video-description)
