# Awesome-Video-Captioning
A curated list of research papers in Video Captioning(from 2015 to 2019). Link to the code and project website if available.

# Paper List
## 2015
**LSTM-P**: [Translating Videos to Natural Language Using Deep Recurrent Neural Networks](https://www.cs.utexas.edu/users/ml/papers/venugopalan.naacl15.pdf) <br>
*Subhashini Venugopalan, Huijuan Xu, Jeff Donahue, Marcus Rohrbach, Raymond Mooney, Kate Saenko* <br>
NAACL, 2015.[[caffe-code]](https://gist.github.com/vsubhashini/3761b9ad43f60db9ac3d)

**LRCN**: [Long-term Recurrent Convolutional Networks for Visual Recognition and Description](https://www.cv-foundation.org/openaccess/content_cvpr_2015/papers/Donahue_Long-Term_Recurrent_Convolutional_2015_CVPR_paper.pdf) <br>
*Jeff Donahue, Lisa Anne Hendricks, Marcus Rohrbach, Subhashini Venugopalan, Sergio Guadarrama, Kate Saenko, Trevor Darrell* <br>
CVPR, 2015.[[website]](http://jeffdonahue.com/lrcn/)

**S2VT**: [Sequence to Sequence – Video to Text](https://www.cs.utexas.edu/users/ml/papers/venugopalan.iccv15.pdf)<br>
*Subhashini Venugopalan, Marcus Rohrbach, Jeff Donahue, Raymond Mooney, Trevor Darrell, Kate Saenko*<br>
ICCV, 2015.[[caffe-code]](https://gist.github.com/vsubhashini/38d087e140854fee4b14)

**SA**: [Describing Videos by Exploiting Temporal Structure](https://www.cv-foundation.org/openaccess/content_iccv_2015/papers/Yao_Describing_Videos_by_ICCV_2015_paper.pdf)<br>
*Li Yao, Atousa Torabi, Kyunghyun Cho, Nicolas Ballas, Christopher Pal, Hugo Larochelle, Aaron Courville*<br>
ICCV, 2015.[[theano-code]](https://github.com/yaoli/arctic-capgen-vid) [[tf-code]](https://github.com/tsenghungchen/SA-tensorflow)

## 2016
**LSTM-E**: [Jointly Modeling Embedding and Translation to Bridge Video and Language](http://openaccess.thecvf.com/content_cvpr_2016/papers/Pan_Jointly_Modeling_Embedding_CVPR_2016_paper.pdf)<br>
*Yingwei Pan, Tao Mei, Ting Yao, Houqiang Li, Yong Rui*<br>
CVPR, 2016.

**HRNE**: [Hierarchical Recurrent Neural Encoder for Video Representation with Application to Captioning](http://zhongwen.ai/pdf/HRNE.pdf)<br>
*Pingbo Pan, Zhongwen Xu, Yi Yang, Fei Wu, Yueting Zhuang*<br>
CVPR, 2016.

**h-RNN**: [Video Paragraph Captioning Using Hierarchical Recurrent Neural Networks](https://arxiv.org/pdf/1510.07712)<br>
*Haonan Yu, Jiang Wang, Zhiheng Huang, Yi Yang, Wei Xu*<br>
CVPR, 2016.

**MSR-VTT**: [MSR-VTT: A Large Video Description Dataset for Bridging Video and Language](https://www.microsoft.com/en-us/research/wp-content/uploads/2016/06/cvpr16.msr-vtt.tmei_-1.pdf)<br>
*Jun Xu , Tao Mei , Ting Yao and Yong Rui*<br>
CVPR, 2016.[[website]](https://www.microsoft.com/en-us/research/publication/msr-vtt-a-large-video-description-dataset-for-bridging-video-and-language/)

**BiLSTM**: [Video Description using Bidirectional Recurrent Neural Networks](https://arxiv.org/pdf/1604.03390)<br>
*Álvaro Peris, Marc Bolaños, Petia Radeva, Francisco Casacuberta*<br>
ICANN, 2016.

## 2017
**DenseVidCap**: [ Weakly Supervised Dense Video Captioning](https://arxiv.org/pdf/1704.01502)<br>
*Zhiqiang Shen, Jianguo Li, Zhou Su, Minjun Li, Yurong Chen, Yu-Gang Jiang, Xiangyang Xue*<br>
CVPR, 2017.[[tf-code]](https://github.com/SCLinDennis/Weakly-Supervised-Dense-Video-Captioning)

**LSTM-TSA**: [Video Captioning with Transferred Semantic Attributes](https://arxiv.org/pdf/1611.07675)<br>
*Yingwei Pan, Ting Yao, Houqiang Li, Tao Mei*<br>
CVPR, 2017.

**SCN**: [Semantic Compositional Networks for Visual Captioning](https://arxiv.org/pdf/1611.08002)<br>
*Zhe Gan, Chuang Gan, Xiaodong He, Yunchen Pu, Kenneth Tran, Jianfeng Gao, Lawrence Carin, Li Deng*<br>
CVPR, 2017.[[theano-code]](https://github.com/zhegan27/Semantic_Compositional_Nets)

**StyleNet**: [StyleNet: Generating Attractive Visual Captions with Styles](http://openaccess.thecvf.com/content_cvpr_2017/papers/Gan_StyleNet_Generating_Attractive_CVPR_2017_paper.pdf)<br>
*Chuang Gan, Zhe Gan, Xiaodong He, Jianfeng Gao, Li Deng*<br>
CVPR, 2017.[[pytorch-code]](https://github.com/kacky24/stylenet)

**CT-SAN**: [End-to-end Concept Word Detection for Video Captioning, Retrieval, and Question Answering](https://zpascal.net/cvpr2017/Yu_End-To-End_Concept_Word_CVPR_2017_paper.pdf)<br>
*Youngjae Yu, Hyungjin Ko, Jongwook Choi, Gunhee Kim*<br>
CVPR, 2017.[[tf-code]](https://gitlab.com/fodrh1201/CT-SAN/tree/master)

**CGVS**: [Top-down Visual Saliency Guided by Captions](http://zpascal.net/cvpr2017/Ramanishka_Top-Down_Visual_Saliency_CVPR_2017_paper.pdf)<br>
*Vasili Ramanishka, Abir Das, Jianming Zhang, Kate Saenko*<br>
CVPR, 2017.[[tf-code]](https://github.com/VisionLearningGroup/caption-guided-saliency)

**BA**: [Hierarchical Boundary-Aware Neural Encoder for Video Captioning](http://openaccess.thecvf.com/content_cvpr_2017/papers/Baraldi_Hierarchical_Boundary-Aware_Neural_CVPR_2017_paper.pdf)<br>
*Lorenzo Baraldi, Costantino Grana, Rita Cucchiara*<br>
CVPR, 2017.[[pytorch-code]](https://github.com/Yugnaynehc/banet)

**TDDF**: [Task-Driven Dynamic Fusion: Reducing Ambiguity in Video Description](https://www.zpascal.net/cvpr2017/Zhang_Task-Driven_Dynamic_Fusion_CVPR_2017_paper.pdf)<br>
*Xishan Zhang, Ke Gao, Yongdong Zhang, Dongming Zhang, Jintao Li,and Qi Tian*<br>
CVPR, 2017.

**GEAN**: [Supervising Neural Attention Models for Video Captioning by Human Gaze Data](http://zpascal.net/cvpr2017/Yu_Supervising_Neural_Attention_CVPR_2017_paper.pdf)<br>
*Youngjae Yu, Jongwook Choi, Yeonhwa Kim, Kyung Yoo, Sang-Hun Lee, Gunhee Kim*<br>
CVPR, 2017.[[tf-code]](https://github.com/yj-yu/Recurrent_Gaze_Prediction)

**MM-Att**: [Attention-Based Multimodal Fusion for Video Description](http://openaccess.thecvf.com/content_ICCV_2017/papers/Hori_Attention-Based_Multimodal_Fusion_ICCV_2017_paper.pdf)<br>
*Chiori Hori, Takaaki Hori, Teng-Yok Lee, Kazuhiro Sumi, John R. Hershey, Tim K. Marks*<br>
ICCV, 2017.

**Tessellation**: [Temporal Tessellation: A Unified Approach for Video Analysis](http://openaccess.thecvf.com/content_ICCV_2017/papers/Kaufman_Temporal_Tessellation_A_ICCV_2017_paper.pdf)<br>
*Dotan Kaufman, Gil Levi, Tal Hassner, Lior Wolf*<br>
ICCV, 2017.[[tf-code]](https://github.com/dot27/temporal-tessellation)

## 2018
**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)

**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


## 2019
**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)

**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


## Dense Captioning
**DenseEvents**: [Dense-Captioning Events in Videos](http://openaccess.thecvf.com/content_ICCV_2017/papers/Krishna_Dense-Captioning_Events_in_ICCV_2017_paper.pdf)<br>
*Ranjay Krishna, Kenji Hata, Frederic Ren, Li Fei-Fei, Juan Carlos Niebles*<br>
ICCV, 2017.[[code]](https://github.com/ranjaykrishna/densevid_eval) [[website]](https://cs.stanford.edu/people/ranjaykrishna/densevid/)

**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)


**suo**: []()<br>
*author*<br>
conf, year.[[code]]() [[web]](link)

## Grounded Captioning
**GVD**: [Grounded Video Description](http://openaccess.thecvf.com/content_CVPR_2019/papers/Zhou_Grounded_Video_Description_CVPR_2019_paper.pdf)<br>
*Luowei Zhou, Yannis Kalantidis, Xinlei Chen, Jason J. Corso, Marcus Rohrbach*<br>
CVPR, 2019.[[pytorch-code]](https://github.com/facebookresearch/grounded-video-description)
